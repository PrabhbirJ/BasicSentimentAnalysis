{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bfb17ac-c902-436d-a631-45168ef20690",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bfb17ac-c902-436d-a631-45168ef20690",
    "outputId": "9ade7d0c-e893-4161-b183-442f709f1e22"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import nltk#for text wrangling\n",
    "import spacy#for text wrangling\n",
    "import re\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e910ec68-130d-4368-9ea0-e4a9ca7d61cc",
   "metadata": {
    "id": "e910ec68-130d-4368-9ea0-e4a9ca7d61cc"
   },
   "outputs": [],
   "source": [
    "age = 'age'\n",
    "s = 'statement'\n",
    "status = 'status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab8521f9-2b58-4c3f-8fc3-93320fc9d664",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "ab8521f9-2b58-4c3f-8fc3-93320fc9d664",
    "outputId": "89fa00b9-501f-4c8a-f449-9ec8cbbffe91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "statement     362\n",
       "status          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old = pd.read_csv(\"sentiments.csv\")\n",
    "df_old.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51650a6-313d-4977-bb7e-f37d9f7999c1",
   "metadata": {
    "id": "b51650a6-313d-4977-bb7e-f37d9f7999c1"
   },
   "outputs": [],
   "source": [
    "#so we have 362 entries where the statements are null. Now as these are text values their imputation is not possible\n",
    "df_new = df_old.copy()\n",
    "df_new.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0b6798-c9ad-4adf-8618-50938d15dfda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "1d0b6798-c9ad-4adf-8618-50938d15dfda",
    "outputId": "24d62161-0161-4c59-abc3-653a58af5082"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Normal                  16343\n",
       "Depression              15404\n",
       "Suicidal                10652\n",
       "Anxiety                  3841\n",
       "Bipolar                  2777\n",
       "Stress                   2587\n",
       "Personality disorder     1077\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e606f2e5-99f8-47dc-a13e-bb9ccceaced2",
   "metadata": {
    "id": "e606f2e5-99f8-47dc-a13e-bb9ccceaced2"
   },
   "outputs": [],
   "source": [
    "#now as we have to train the model on the textual data ie the statements but well we can not input words directly to our model\n",
    "#for that firstly we will do some preprocessing on the text and then convert the words into some vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bad15d8-0cd7-4486-b252-f3dfa5059ed2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "1bad15d8-0cd7-4486-b252-f3dfa5059ed2",
    "outputId": "a19d05e0-3b07-4233-d811-940a72fdc1ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_new['statement']#this will be the input to the model\n",
    "Y = df_new['status']#this is the labels for all our inputs\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed01fb7-e74e-401c-83cb-f6500071ba28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "8ed01fb7-e74e-401c-83cb-f6500071ba28",
    "outputId": "cae58a78-ea1a-42e6-bec7-a0fc0633a3e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               oh my gosh\n",
       "1        trouble sleeping, confused mind, restless hear...\n",
       "2        all wrong, back off dear, forward doubt. stay ...\n",
       "3        i've shifted my focus to something else but i'...\n",
       "4        i'm restless and restless, it's been a month n...\n",
       "                               ...                        \n",
       "53038    nobody takes me seriously i’ve (24m) dealt wit...\n",
       "53039    selfishness  \"i don't feel very good, it's lik...\n",
       "53040    is there any way to sleep better? i can't slee...\n",
       "53041    public speaking tips? hi, all. i have to give ...\n",
       "53042    i have really bad door anxiety! it's not about...\n",
       "Name: statement, Length: 52681, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text wrangling\n",
    "#step 1 lowering all the words to ensure same words with different cases are not considered different\n",
    "X = X.str.lower()#converted the data to string and then applied lower case conversion method\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c5a68e-258a-4f38-8fcd-12f8d6cc8beb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "36c5a68e-258a-4f38-8fcd-12f8d6cc8beb",
    "outputId": "6dda6e7f-7aa2-4ca6-8c59-5dd260a39970"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        anxiety\n",
       "1        anxiety\n",
       "2        anxiety\n",
       "3        anxiety\n",
       "4        anxiety\n",
       "          ...   \n",
       "53038    anxiety\n",
       "53039    anxiety\n",
       "53040    anxiety\n",
       "53041    anxiety\n",
       "53042    anxiety\n",
       "Name: status, Length: 52681, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y.str.lower()\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0410fa2f-bd5e-4522-bc51-0a2537f1d940",
   "metadata": {
    "id": "0410fa2f-bd5e-4522-bc51-0a2537f1d940"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "#step 2\n",
    "def remove_accented_chars(text):\n",
    "    #with this function we will remove the accented characters and replace them with their english letter\n",
    "    #here we are considering that all this would have happened by error and not meant for dealing with other\n",
    "    #languages.\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    #NFKD stand for Normalization Form Compatibility Decomposition which is a unicode normalization method.\n",
    "    #what this line does is it decomposes the accented characters lets say é to e and \"´\"\n",
    "    #then we encode this using ascii which ignores non ascii characters like \"´\"\n",
    "    #finally we decode the ascii encodings to strings using utf-8\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89ed5d-1afd-47ba-97ce-0d7d3187763f",
   "metadata": {
    "id": "ed89ed5d-1afd-47ba-97ce-0d7d3187763f"
   },
   "outputs": [],
   "source": [
    "X = X.apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6341df-1268-4fb5-bb99-4b159f51af32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "7d6341df-1268-4fb5-bb99-4b159f51af32",
    "outputId": "01771d17-ec85-40c2-c8eb-60fabeec2929"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7e44a3-0d04-4edb-92db-9d47a1b43dee",
   "metadata": {
    "id": "da7e44a3-0d04-4edb-92db-9d47a1b43dee"
   },
   "outputs": [],
   "source": [
    "#now we generally remove numbers. As we can see in our data the numbers are used for various purposes\n",
    "#rarely for ages sometimes for a times period like say 20's\n",
    "#we can see that removing them is good for our model\n",
    "#but as this is a simpler model we are removing all numbers well in further devs it is important to use some\n",
    "#context for example in GPT-4 4 should not be removed but for our purposes it is well and okay\n",
    "#step 3\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-Z\\s]' #so anything that is not a letter or a space is removed\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1efb39-a785-4609-bed7-412ef220f43f",
   "metadata": {
    "id": "7c1efb39-a785-4609-bed7-412ef220f43f"
   },
   "outputs": [],
   "source": [
    "X=X.apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc06e4-4003-4659-84f2-7e4c0c2ff940",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75fc06e4-4003-4659-84f2-7e4c0c2ff940",
    "outputId": "b9da3e02-dc2a-4cd3-fd48-94f4ea1d8888"
   },
   "outputs": [],
   "source": [
    "#next we want to expand contractions\n",
    "#don't will be do not and so on for the entire dataset\n",
    "contractions.contractions_dict.items()\n",
    "#so what contractions do is it has a dictionary contractions_dict where the key is the contraction\n",
    "#and the value is expansion of that contraction so nothing really fancy about how it works\n",
    "#now using this dictionary one word by another and stuff that's boring right. well!\n",
    "#contractions provide a function fix for that same purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956647e6-f126-4392-993b-393aa6aafed8",
   "metadata": {
    "id": "956647e6-f126-4392-993b-393aa6aafed8"
   },
   "outputs": [],
   "source": [
    "#step 4\n",
    "X = X.apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d3926-da1c-4e7f-b89e-b166dac7a64e",
   "metadata": {
    "id": "5c9d3926-da1c-4e7f-b89e-b166dac7a64e"
   },
   "outputs": [],
   "source": [
    "#here in our case we are not doing things like pos tagging or chunking\n",
    "#well we are really not interested in the grammar\n",
    "#there might be a reason to do this if we might reach to a conclusion that depressed or other people might\n",
    "#write smaller text use more adjectives etc. Well not really. We will use a neural network that will deal\n",
    "#with the semantics rather than these syntax things which might not add much to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcc2eb-f319-4205-9c88-75edfdde1748",
   "metadata": {
    "id": "afdcc2eb-f319-4205-9c88-75edfdde1748"
   },
   "outputs": [],
   "source": [
    "#step 5 stemming or lemmatization\n",
    "#well the words depressed depression and depressing has the same root word that we care about\n",
    "#in our dataset they might be treated differently\n",
    "#and we want to preserve the relations inspite of the morphological differences\n",
    "#we can also perform stemming here but stemming is not as accurate as lemmatization\n",
    "#lemmatization can be slower than stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a21da4-07de-4139-bcd0-047abd0d93b6",
   "metadata": {
    "id": "21a21da4-07de-4139-bcd0-047abd0d93b6"
   },
   "outputs": [],
   "source": [
    "# Load spaCy's large English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "def lemmatize_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if token.lemma_.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823b3d7-e1dc-48de-89f7-96f108ed7acd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "7823b3d7-e1dc-48de-89f7-96f108ed7acd",
    "outputId": "dc262874-92fc-427a-ecee-52e6714adf94"
   },
   "outputs": [],
   "source": [
    "X = X.apply(lemmatize_with_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f3075-7bd4-4ea8-99f9-fae7e1e59c96",
   "metadata": {
    "id": "d04f3075-7bd4-4ea8-99f9-fae7e1e59c96"
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "NEGATION_WORDS = {\"not\", \"no\", \"never\", \"none\", \"nor\", \"nothing\", \"nowhere\", \"hardly\", \"scarcely\", \"barely\"}\n",
    "\n",
    "# Remove stopwords but keep negation words\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords except for important negation words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word.lower() not in STOPWORDS or word.lower() in NEGATION_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd60cf-fb9e-48cb-8110-2343efee91d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "affd60cf-fb9e-48cb-8110-2343efee91d7",
    "outputId": "dcc1ad90-915d-4855-c607-5708c17e055b"
   },
   "outputs": [],
   "source": [
    "X = X.apply(remove_stopwords)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39da37-0fe8-4edf-aaf1-84992cba89a4",
   "metadata": {
    "id": "2a39da37-0fe8-4edf-aaf1-84992cba89a4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#step 6 that is tokenization\n",
    "# Apply tokenization only\n",
    "X = X.astype(str).apply(lambda x: [token.text for token in nlp.tokenizer(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08322b65-f94f-4db8-b5fe-1e1d42c3d6aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "08322b65-f94f-4db8-b5fe-1e1d42c3d6aa",
    "outputId": "193c7777-6961-4ce5-814a-d50b8ff644db"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "328fdcf8-7537-419d-bc12-edbb9fa5f21d",
   "metadata": {
    "id": "328fdcf8-7537-419d-bc12-edbb9fa5f21d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)  # remove special chars\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Use token.text instead of token.lemma_ to avoid lemmatization\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "X = X.apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676d261-01a6-4bf5-b407-713a3bdfe316",
   "metadata": {
    "id": "2676d261-01a6-4bf5-b407-713a3bdfe316"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "def vocabulary_indices(tokens, min_freq=1):\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(tokens)\n",
    "\n",
    "    # Filter words that meet the minimum frequency threshold\n",
    "    vocabulary = {word for word, count in word_counts.items() if count >= min_freq}\n",
    "\n",
    "    # Sort for consistent indexing\n",
    "    sorted_vocab = sorted(vocabulary)  # Alphabetical order (can also sort by frequency)\n",
    "\n",
    "    # Create mappings\n",
    "    word2index = {word: i for i, word in enumerate(sorted_vocab)}\n",
    "    index2word = {i: word for i, word in enumerate(sorted_vocab)}\n",
    "\n",
    "    return vocabulary, word2index, index2word\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27220f-6670-4d4b-8570-dae9d55c8c16",
   "metadata": {
    "id": "1e27220f-6670-4d4b-8570-dae9d55c8c16"
   },
   "outputs": [],
   "source": [
    "#hyperparameters:\n",
    "#window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186f52f-ce49-4f1f-b461-394142d2ae1a",
   "metadata": {
    "id": "c186f52f-ce49-4f1f-b461-394142d2ae1a"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Generate context pairs for each sentence (flattened list)\n",
    "all_context_pairs = X.apply(lambda tokens: generate_context_pairs(tokens, window_size))\n",
    "\n",
    "# Ensure we remove any None or empty lists\n",
    "context_pairs = [pair for sublist in all_context_pairs.dropna() for pair in sublist]\n",
    "\n",
    "# Ensure X is fully tokenized before building vocab\n",
    "if isinstance(X.iloc[0], str):  # If X contains raw text instead of tokenized words\n",
    "    raise ValueError(\"X contains raw text; it must be tokenized into lists of words first.\")\n",
    "\n",
    "# Collect all words for vocabulary (with repetitions)\n",
    "all_vocab = [word for tokens in X for word in tokens]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab, w2i, i2w = vocabulary_indices(all_vocab)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584665d-23eb-43d7-87de-49fd57c13b48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7584665d-23eb-43d7-87de-49fd57c13b48",
    "outputId": "00287944-3613-4caa-a76b-6f248d767296"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dbff7-082b-4e8f-95f0-82450282f07b",
   "metadata": {
    "id": "2c4dbff7-082b-4e8f-95f0-82450282f07b"
   },
   "outputs": [],
   "source": [
    "'''unknown_idx = w2i.get(\"<UNK>\", 0)  # Fallback index for unknown words\n",
    "X_training = [(w2i.get(target, unknown_idx), w2i.get(context, unknown_idx)) for target, context in context_pairs]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642cd77d-06b2-4d22-9209-0e651381a465",
   "metadata": {
    "id": "642cd77d-06b2-4d22-9209-0e651381a465"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, learning_rate, epochs, batch_size, neg_samples=5):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.V = vocab_size  # Vocabulary size\n",
    "        self.N = embedding_dim  # Embedding dimension\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.neg_samples = neg_samples\n",
    "\n",
    "        # Word embeddings\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Initialize embeddings using uniform distribution\n",
    "        self.in_embed.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "        self.out_embed.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "\n",
    "        # Optimizer & learning rate scheduler\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "    def forward(self, target, context, neg_context):\n",
    "        \"\"\"\n",
    "        Compute the SkipGram loss using:\n",
    "        - Positive samples (target, context)\n",
    "        - Negative samples (target, neg_context)\n",
    "        \"\"\"\n",
    "        # Positive pairs\n",
    "        target_embeds = self.in_embed(target)  # (batch_size, embedding_dim)\n",
    "        context_embeds = self.out_embed(context)  # (batch_size, embedding_dim)\n",
    "        pos_score = torch.mul(target_embeds, context_embeds).sum(dim=1)\n",
    "        pos_loss = F.logsigmoid(pos_score).squeeze()\n",
    "\n",
    "        # Negative pairs\n",
    "        neg_context_embeds = self.out_embed(neg_context)  # (batch_size, neg_samples, embedding_dim)\n",
    "        neg_score = torch.bmm(neg_context_embeds, target_embeds.unsqueeze(2)).squeeze()\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)  # Sum over negative samples\n",
    "\n",
    "        return -(pos_loss + neg_loss).mean()  # Negative log likelihood\n",
    "\n",
    "    def train_model(self, dataset):\n",
    "        \"\"\"\n",
    "        Train the SkipGram model using the given dataset.\n",
    "        \"\"\"\n",
    "        data_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        device = self.get_best_device()\n",
    "        self.to(device)\n",
    "        print(f\"🚀 Model initialized on: {device}\")\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs} on {device}\")\n",
    "\n",
    "            for batch_idx, (target_batch, context_batch, neg_batch) in enumerate(data_loader):\n",
    "                target_batch = target_batch.to(device)\n",
    "                context_batch = context_batch.to(device)\n",
    "                neg_batch = neg_batch.to(device)  # Negative samples already computed\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.forward(target_batch, context_batch, neg_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if batch_idx % 100 == 0:\n",
    "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                if batch_idx%10000==0:\n",
    "                    print(f\"  Batch {batch_idx}: Loss = {loss.item():.4f}, LR = {current_lr:.6f}\")\n",
    "\n",
    "            avg_loss = total_loss / len(data_loader)\n",
    "            self.scheduler.step(avg_loss)\n",
    "            print(f\"✅ Epoch {epoch+1}/{self.epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_best_device():\n",
    "        \"\"\"\n",
    "        Automatically picks the best available device (CUDA → MPS → CPU).\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GYRBnfaUxMZY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "GYRBnfaUxMZY",
    "outputId": "26d8802e-8025-4901-cbda-8b1cce172784"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def prepare_dataset(X_training, vocab_size, neg_samples=5):\n",
    "    print(\"🔹 Preparing dataset with negative sampling...\")\n",
    "    print(f\"  - Total training pairs: {len(X_training)}\")\n",
    "    print(f\"  - Vocabulary size: {vocab_size}\")\n",
    "    print(f\"  - Negative samples per word: {neg_samples}\")\n",
    "\n",
    "    targets, contexts = zip(*X_training)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    contexts = torch.tensor(contexts, dtype=torch.long)\n",
    "\n",
    "    all_vocab = list(range(vocab_size))  # List of all words in vocabulary\n",
    "\n",
    "    neg_contexts = []\n",
    "    for i in range(len(targets)):\n",
    "        negative_samples = []\n",
    "        while len(negative_samples) < neg_samples:\n",
    "            neg_word = random.choice(all_vocab)\n",
    "            if neg_word != contexts[i].item():  # Ensure it's not the actual context\n",
    "                negative_samples.append(neg_word)\n",
    "        \n",
    "        neg_contexts.append(torch.tensor(negative_samples, dtype=torch.long))\n",
    "        \n",
    "        \n",
    "\n",
    "    neg_contexts = torch.stack(neg_contexts)\n",
    "\n",
    "    print(\"✅ Dataset preparation complete!\\n\")\n",
    "    return TensorDataset(targets, contexts, neg_contexts)\n",
    "dataset = prepare_dataset(X_training, vocab_size, neg_samples=10)\n",
    "# Example usage:\n",
    "# X_training = [(target_word, context_word), ...]\n",
    "# vocab_size = total number of unique words\n",
    "# dataset = prepare_dataset(X_training, vocab_size, neg_samples=5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353f4a5-ef63-459d-bd5e-390a197bfcd2",
   "metadata": {
    "id": "6353f4a5-ef63-459d-bd5e-390a197bfcd2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "embedding_dim = 150  # You can reduce this to 50-300 typically\n",
    "learning_rate = 0.001  # Reduced from 0.01\n",
    "epochs = 6\n",
    "batch_size = 64\n",
    "neg_samples = 10  # Number of negative samples per positive sample\n",
    "\n",
    "model = SkipGramModel(vocab_size, embedding_dim, learning_rate, epochs, batch_size, neg_samples)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea9fc9-b186-41bb-a52c-1680099a1c2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "6aea9fc9-b186-41bb-a52c-1680099a1c2a",
    "outputId": "19a9a382-bd96-4a62-c18c-fc107de8b143"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984c3a0-26bc-4fd4-9e3b-a2521726ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def get_similar_words(model, word, word_to_index, index_to_word, top_n=5):\n",
    "    \"\"\" Get top N most similar words based on cosine similarity \"\"\"\n",
    "    word_index = word_to_index.get(word, None)\n",
    "    if word_index is None:\n",
    "        print(f\"Word '{word}' not in vocabulary.\")\n",
    "        return []\n",
    "\n",
    "    word_embedding = model.in_embed.weight[word_index].detach().cpu()\n",
    "    embeddings = model.in_embed.weight.detach().cpu()\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = torch.cosine_similarity(word_embedding.unsqueeze(0), embeddings, dim=1)\n",
    "    closest_indices = similarities.argsort(descending=True)[1:top_n+1]  # Ignore itself\n",
    "\n",
    "    similar_words = [index_to_word[idx.item()] for idx in closest_indices]\n",
    "    return similar_words'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26186c7-3368-46c5-9022-02a6d72c21a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca6fafcf-2d53-401c-8850-6f3c42e3b238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:07:48,380 : INFO : collecting all words and their counts\n",
      "2025-04-09 16:07:48,381 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-04-09 16:07:48,437 : INFO : PROGRESS: at sentence #10000, processed 604345 words, keeping 20360 word types\n",
      "2025-04-09 16:07:48,580 : INFO : PROGRESS: at sentence #20000, processed 2299761 words, keeping 33109 word types\n",
      "2025-04-09 16:07:48,697 : INFO : PROGRESS: at sentence #30000, processed 3755918 words, keeping 40969 word types\n",
      "2025-04-09 16:07:48,767 : INFO : PROGRESS: at sentence #40000, processed 4612728 words, keeping 46690 word types\n",
      "2025-04-09 16:07:48,846 : INFO : PROGRESS: at sentence #50000, processed 5596126 words, keeping 55172 word types\n",
      "2025-04-09 16:07:48,883 : INFO : collected 56239 word types from a corpus of 6067715 raw words and 52681 sentences\n",
      "2025-04-09 16:07:48,883 : INFO : Creating a fresh vocabulary\n",
      "2025-04-09 16:07:48,909 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 21323 unique words (37.91% of original 56239, drops 34916)', 'datetime': '2025-04-09T16:07:48.909916', 'gensim': '4.3.3', 'python': '3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.3.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-04-09 16:07:48,910 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 6025909 word corpus (99.31% of original 6067715, drops 41806)', 'datetime': '2025-04-09T16:07:48.910184', 'gensim': '4.3.3', 'python': '3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.3.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-04-09 16:07:48,945 : INFO : deleting the raw counts dictionary of 56239 items\n",
      "2025-04-09 16:07:48,946 : INFO : sample=0.001 downsamples 60 most-common words\n",
      "2025-04-09 16:07:48,946 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4149464.047197307 word corpus (68.9%% of prior 6025909)', 'datetime': '2025-04-09T16:07:48.946374', 'gensim': '4.3.3', 'python': '3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.3.2-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-04-09 16:07:49,005 : INFO : estimated required memory for 21323 words and 100 dimensions: 27719900 bytes\n",
      "2025-04-09 16:07:49,005 : INFO : resetting layer weights\n",
      "2025-04-09 16:07:49,011 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-09T16:07:49.011831', 'gensim': '4.3.3', 'python': '3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.3.2-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2025-04-09 16:07:49,012 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 21323 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-04-09T16:07:49.012041', 'gensim': '4.3.3', 'python': '3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.3.2-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-04-09 16:07:50,024 : INFO : EPOCH 0 - PROGRESS: at 27.58% examples, 927015 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:07:51,030 : INFO : EPOCH 0 - PROGRESS: at 42.19% examples, 892750 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:07:52,036 : INFO : EPOCH 0 - PROGRESS: at 64.77% examples, 859854 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:07:53,042 : INFO : EPOCH 0 - PROGRESS: at 87.68% examples, 837299 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:07:54,055 : INFO : EPOCH 0 - PROGRESS: at 99.51% examples, 817498 words/s, in_qsize 5, out_qsize 1\n",
      "2025-04-09 16:07:54,074 : INFO : EPOCH 0: training on 6067715 raw words (4149374 effective words) took 5.1s, 820299 effective words/s\n",
      "2025-04-09 16:07:55,089 : INFO : EPOCH 1 - PROGRESS: at 24.18% examples, 722436 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:07:56,090 : INFO : EPOCH 1 - PROGRESS: at 36.98% examples, 742633 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:07:57,095 : INFO : EPOCH 1 - PROGRESS: at 49.46% examples, 741422 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:07:58,114 : INFO : EPOCH 1 - PROGRESS: at 74.09% examples, 749905 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:07:59,153 : INFO : EPOCH 1 - PROGRESS: at 94.71% examples, 751495 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:07:59,574 : INFO : EPOCH 1: training on 6067715 raw words (4149254 effective words) took 5.5s, 755346 effective words/s\n",
      "2025-04-09 16:08:00,581 : INFO : EPOCH 2 - PROGRESS: at 24.35% examples, 734956 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:01,592 : INFO : EPOCH 2 - PROGRESS: at 37.34% examples, 751786 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:08:02,600 : INFO : EPOCH 2 - PROGRESS: at 50.30% examples, 755555 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:08:03,600 : INFO : EPOCH 2 - PROGRESS: at 74.65% examples, 761341 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:04,601 : INFO : EPOCH 2 - PROGRESS: at 94.71% examples, 759663 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:05,030 : INFO : EPOCH 2: training on 6067715 raw words (4151310 effective words) took 5.4s, 761777 effective words/s\n",
      "2025-04-09 16:08:06,051 : INFO : EPOCH 3 - PROGRESS: at 24.69% examples, 744632 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:07,061 : INFO : EPOCH 3 - PROGRESS: at 37.34% examples, 747267 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:08,065 : INFO : EPOCH 3 - PROGRESS: at 50.30% examples, 753332 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:09,067 : INFO : EPOCH 3 - PROGRESS: at 74.00% examples, 748874 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:10,079 : INFO : EPOCH 3 - PROGRESS: at 93.93% examples, 746670 words/s, in_qsize 16, out_qsize 3\n",
      "2025-04-09 16:08:10,572 : INFO : EPOCH 3: training on 6067715 raw words (4150337 effective words) took 5.5s, 749822 effective words/s\n",
      "2025-04-09 16:08:11,581 : INFO : EPOCH 4 - PROGRESS: at 24.30% examples, 732950 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:12,593 : INFO : EPOCH 4 - PROGRESS: at 36.43% examples, 724090 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:13,600 : INFO : EPOCH 4 - PROGRESS: at 48.53% examples, 721782 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:14,608 : INFO : EPOCH 4 - PROGRESS: at 72.42% examples, 723177 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:15,615 : INFO : EPOCH 4 - PROGRESS: at 91.96% examples, 724279 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:16,292 : INFO : EPOCH 4: training on 6067715 raw words (4149853 effective words) took 5.7s, 726330 effective words/s\n",
      "2025-04-09 16:08:17,316 : INFO : EPOCH 5 - PROGRESS: at 23.74% examples, 689801 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:18,318 : INFO : EPOCH 5 - PROGRESS: at 35.80% examples, 702573 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:19,322 : INFO : EPOCH 5 - PROGRESS: at 47.51% examples, 701452 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:20,355 : INFO : EPOCH 5 - PROGRESS: at 71.52% examples, 703259 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:21,360 : INFO : EPOCH 5 - PROGRESS: at 90.45% examples, 702012 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:22,218 : INFO : EPOCH 5: training on 6067715 raw words (4150144 effective words) took 5.9s, 701105 effective words/s\n",
      "2025-04-09 16:08:23,232 : INFO : EPOCH 6 - PROGRESS: at 23.50% examples, 683741 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:24,261 : INFO : EPOCH 6 - PROGRESS: at 35.01% examples, 673876 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:08:25,264 : INFO : EPOCH 6 - PROGRESS: at 46.22% examples, 673933 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:26,281 : INFO : EPOCH 6 - PROGRESS: at 66.81% examples, 675977 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:27,284 : INFO : EPOCH 6 - PROGRESS: at 88.37% examples, 674332 words/s, in_qsize 16, out_qsize 1\n",
      "2025-04-09 16:08:28,289 : INFO : EPOCH 6 - PROGRESS: at 98.50% examples, 669153 words/s, in_qsize 13, out_qsize 1\n",
      "2025-04-09 16:08:28,386 : INFO : EPOCH 6: training on 6067715 raw words (4150072 effective words) took 6.2s, 673567 effective words/s\n",
      "2025-04-09 16:08:29,410 : INFO : EPOCH 7 - PROGRESS: at 22.85% examples, 638553 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:30,418 : INFO : EPOCH 7 - PROGRESS: at 33.96% examples, 648325 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:08:31,419 : INFO : EPOCH 7 - PROGRESS: at 44.58% examples, 644076 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:32,424 : INFO : EPOCH 7 - PROGRESS: at 64.97% examples, 647508 words/s, in_qsize 16, out_qsize 1\n",
      "2025-04-09 16:08:33,447 : INFO : EPOCH 7 - PROGRESS: at 86.18% examples, 645046 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:08:34,458 : INFO : EPOCH 7 - PROGRESS: at 96.42% examples, 646237 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:34,803 : INFO : EPOCH 7: training on 6067715 raw words (4147857 effective words) took 6.4s, 647188 effective words/s\n",
      "2025-04-09 16:08:35,821 : INFO : EPOCH 8 - PROGRESS: at 22.37% examples, 609841 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:36,824 : INFO : EPOCH 8 - PROGRESS: at 32.82% examples, 615890 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:08:37,827 : INFO : EPOCH 8 - PROGRESS: at 43.00% examples, 613131 words/s, in_qsize 16, out_qsize 2\n",
      "2025-04-09 16:08:38,835 : INFO : EPOCH 8 - PROGRESS: at 55.26% examples, 617744 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:39,873 : INFO : EPOCH 8 - PROGRESS: at 75.72% examples, 618068 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:40,876 : INFO : EPOCH 8 - PROGRESS: at 93.55% examples, 617436 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:41,521 : INFO : EPOCH 8: training on 6067715 raw words (4150584 effective words) took 6.7s, 618530 effective words/s\n",
      "2025-04-09 16:08:42,536 : INFO : EPOCH 9 - PROGRESS: at 22.06% examples, 593375 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:43,547 : INFO : EPOCH 9 - PROGRESS: at 32.36% examples, 602088 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:44,558 : INFO : EPOCH 9 - PROGRESS: at 42.28% examples, 596527 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:08:45,565 : INFO : EPOCH 9 - PROGRESS: at 53.14% examples, 598977 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:46,565 : INFO : EPOCH 9 - PROGRESS: at 74.09% examples, 601030 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:47,587 : INFO : EPOCH 9 - PROGRESS: at 91.69% examples, 600094 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:48,416 : INFO : EPOCH 9: training on 6067715 raw words (4150236 effective words) took 6.9s, 602867 effective words/s\n",
      "2025-04-09 16:08:49,428 : INFO : EPOCH 10 - PROGRESS: at 21.69% examples, 567790 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:50,439 : INFO : EPOCH 10 - PROGRESS: at 31.39% examples, 576474 words/s, in_qsize 14, out_qsize 0\n",
      "2025-04-09 16:08:51,461 : INFO : EPOCH 10 - PROGRESS: at 41.37% examples, 577055 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:52,487 : INFO : EPOCH 10 - PROGRESS: at 51.33% examples, 576252 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:53,503 : INFO : EPOCH 10 - PROGRESS: at 72.59% examples, 576634 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:54,511 : INFO : EPOCH 10 - PROGRESS: at 90.05% examples, 579613 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:55,521 : INFO : EPOCH 10 - PROGRESS: at 99.10% examples, 576821 words/s, in_qsize 8, out_qsize 1\n",
      "2025-04-09 16:08:55,587 : INFO : EPOCH 10: training on 6067715 raw words (4150205 effective words) took 7.2s, 579527 effective words/s\n",
      "2025-04-09 16:08:56,603 : INFO : EPOCH 11 - PROGRESS: at 21.49% examples, 555446 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:57,628 : INFO : EPOCH 11 - PROGRESS: at 30.89% examples, 558549 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:58,664 : INFO : EPOCH 11 - PROGRESS: at 40.90% examples, 562273 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:08:59,689 : INFO : EPOCH 11 - PROGRESS: at 50.77% examples, 563581 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:00,696 : INFO : EPOCH 11 - PROGRESS: at 71.65% examples, 567113 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:01,734 : INFO : EPOCH 11 - PROGRESS: at 89.30% examples, 566790 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:02,748 : INFO : EPOCH 11 - PROGRESS: at 98.61% examples, 568194 words/s, in_qsize 13, out_qsize 0\n",
      "2025-04-09 16:09:02,881 : INFO : EPOCH 11: training on 6067715 raw words (4147775 effective words) took 7.3s, 569521 effective words/s\n",
      "2025-04-09 16:09:03,895 : INFO : EPOCH 12 - PROGRESS: at 21.70% examples, 567258 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:09:04,896 : INFO : EPOCH 12 - PROGRESS: at 30.82% examples, 562005 words/s, in_qsize 13, out_qsize 2\n",
      "2025-04-09 16:09:05,900 : INFO : EPOCH 12 - PROGRESS: at 40.59% examples, 566001 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:06,925 : INFO : EPOCH 12 - PROGRESS: at 50.04% examples, 561731 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:07,957 : INFO : EPOCH 12 - PROGRESS: at 71.73% examples, 565402 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:09,004 : INFO : EPOCH 12 - PROGRESS: at 89.20% examples, 567891 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:10,008 : INFO : EPOCH 12 - PROGRESS: at 98.38% examples, 569027 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:10,143 : INFO : EPOCH 12: training on 6067715 raw words (4149768 effective words) took 7.3s, 572057 effective words/s\n",
      "2025-04-09 16:09:11,154 : INFO : EPOCH 13 - PROGRESS: at 21.27% examples, 545324 words/s, in_qsize 14, out_qsize 2\n",
      "2025-04-09 16:09:12,156 : INFO : EPOCH 13 - PROGRESS: at 31.16% examples, 573021 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:13,158 : INFO : EPOCH 13 - PROGRESS: at 40.68% examples, 569500 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:14,164 : INFO : EPOCH 13 - PROGRESS: at 50.67% examples, 573596 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:15,165 : INFO : EPOCH 13 - PROGRESS: at 72.23% examples, 578759 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:16,166 : INFO : EPOCH 13 - PROGRESS: at 89.49% examples, 581041 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:17,172 : INFO : EPOCH 13 - PROGRESS: at 98.61% examples, 579161 words/s, in_qsize 12, out_qsize 1\n",
      "2025-04-09 16:09:17,274 : INFO : EPOCH 13: training on 6067715 raw words (4150001 effective words) took 7.1s, 582845 effective words/s\n",
      "2025-04-09 16:09:18,289 : INFO : EPOCH 14 - PROGRESS: at 21.56% examples, 561118 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:19,295 : INFO : EPOCH 14 - PROGRESS: at 31.27% examples, 573893 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:20,298 : INFO : EPOCH 14 - PROGRESS: at 40.99% examples, 574537 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:21,318 : INFO : EPOCH 14 - PROGRESS: at 50.67% examples, 570325 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:22,331 : INFO : EPOCH 14 - PROGRESS: at 71.63% examples, 566540 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:09:23,339 : INFO : EPOCH 14 - PROGRESS: at 88.44% examples, 564878 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:24,345 : INFO : EPOCH 14 - PROGRESS: at 97.30% examples, 564030 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:24,653 : INFO : EPOCH 14: training on 6067715 raw words (4149815 effective words) took 7.4s, 563136 effective words/s\n",
      "2025-04-09 16:09:25,671 : INFO : EPOCH 15 - PROGRESS: at 21.18% examples, 533928 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:26,685 : INFO : EPOCH 15 - PROGRESS: at 30.47% examples, 547554 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:27,703 : INFO : EPOCH 15 - PROGRESS: at 40.25% examples, 554010 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:28,711 : INFO : EPOCH 15 - PROGRESS: at 49.35% examples, 550146 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:29,722 : INFO : EPOCH 15 - PROGRESS: at 67.34% examples, 548680 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:09:30,742 : INFO : EPOCH 15 - PROGRESS: at 86.64% examples, 541889 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:09:31,761 : INFO : EPOCH 15 - PROGRESS: at 95.48% examples, 543611 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:32,249 : INFO : EPOCH 15: training on 6067715 raw words (4149207 effective words) took 7.6s, 546829 effective words/s\n",
      "2025-04-09 16:09:33,261 : INFO : EPOCH 16 - PROGRESS: at 21.18% examples, 536971 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:34,279 : INFO : EPOCH 16 - PROGRESS: at 30.70% examples, 554490 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:09:35,292 : INFO : EPOCH 16 - PROGRESS: at 39.33% examples, 537657 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:36,296 : INFO : EPOCH 16 - PROGRESS: at 48.23% examples, 534932 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:37,313 : INFO : EPOCH 16 - PROGRESS: at 66.60% examples, 539493 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:38,314 : INFO : EPOCH 16 - PROGRESS: at 86.83% examples, 546298 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:39,324 : INFO : EPOCH 16 - PROGRESS: at 95.82% examples, 549042 words/s, in_qsize 13, out_qsize 2\n",
      "2025-04-09 16:09:39,740 : INFO : EPOCH 16: training on 6067715 raw words (4149434 effective words) took 7.5s, 554525 effective words/s\n",
      "2025-04-09 16:09:40,754 : INFO : EPOCH 17 - PROGRESS: at 21.39% examples, 549429 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:41,758 : INFO : EPOCH 17 - PROGRESS: at 30.82% examples, 561492 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:42,764 : INFO : EPOCH 17 - PROGRESS: at 40.79% examples, 569734 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:43,764 : INFO : EPOCH 17 - PROGRESS: at 50.67% examples, 573084 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:44,775 : INFO : EPOCH 17 - PROGRESS: at 72.21% examples, 576942 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:45,776 : INFO : EPOCH 17 - PROGRESS: at 89.39% examples, 578417 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:46,796 : INFO : EPOCH 17 - PROGRESS: at 98.14% examples, 572844 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:46,988 : INFO : EPOCH 17: training on 6067715 raw words (4149604 effective words) took 7.2s, 573137 effective words/s\n",
      "2025-04-09 16:09:48,012 : INFO : EPOCH 18 - PROGRESS: at 21.06% examples, 525614 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:09:49,051 : INFO : EPOCH 18 - PROGRESS: at 30.47% examples, 539831 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:50,086 : INFO : EPOCH 18 - PROGRESS: at 39.10% examples, 524384 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:51,089 : INFO : EPOCH 18 - PROGRESS: at 48.19% examples, 528298 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:52,089 : INFO : EPOCH 18 - PROGRESS: at 66.70% examples, 537291 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:53,099 : INFO : EPOCH 18 - PROGRESS: at 86.75% examples, 541403 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:54,107 : INFO : EPOCH 18 - PROGRESS: at 95.72% examples, 544888 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:54,580 : INFO : EPOCH 18: training on 6067715 raw words (4149455 effective words) took 7.6s, 547334 effective words/s\n",
      "2025-04-09 16:09:55,594 : INFO : EPOCH 19 - PROGRESS: at 20.66% examples, 503951 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:56,624 : INFO : EPOCH 19 - PROGRESS: at 29.86% examples, 528796 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:57,656 : INFO : EPOCH 19 - PROGRESS: at 39.33% examples, 532549 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:09:58,670 : INFO : EPOCH 19 - PROGRESS: at 48.34% examples, 531512 words/s, in_qsize 16, out_qsize 2\n",
      "2025-04-09 16:09:59,724 : INFO : EPOCH 19 - PROGRESS: at 66.70% examples, 532857 words/s, in_qsize 16, out_qsize 1\n",
      "2025-04-09 16:10:00,741 : INFO : EPOCH 19 - PROGRESS: at 86.18% examples, 530274 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:01,758 : INFO : EPOCH 19 - PROGRESS: at 93.82% examples, 524364 words/s, in_qsize 15, out_qsize 1\n",
      "2025-04-09 16:10:02,500 : INFO : EPOCH 19: training on 6067715 raw words (4149578 effective words) took 7.9s, 524653 effective words/s\n",
      "2025-04-09 16:10:03,509 : INFO : EPOCH 20 - PROGRESS: at 20.35% examples, 485454 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:04,510 : INFO : EPOCH 20 - PROGRESS: at 28.57% examples, 497040 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:05,521 : INFO : EPOCH 20 - PROGRESS: at 37.11% examples, 497511 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:06,524 : INFO : EPOCH 20 - PROGRESS: at 45.79% examples, 503473 words/s, in_qsize 13, out_qsize 2\n",
      "2025-04-09 16:10:07,539 : INFO : EPOCH 20 - PROGRESS: at 56.33% examples, 501080 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:08,540 : INFO : EPOCH 20 - PROGRESS: at 74.00% examples, 500388 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:09,548 : INFO : EPOCH 20 - PROGRESS: at 89.72% examples, 498136 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:10:10,558 : INFO : EPOCH 20 - PROGRESS: at 97.20% examples, 493978 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:10,910 : INFO : EPOCH 20: training on 6067715 raw words (4150209 effective words) took 8.4s, 494013 effective words/s\n",
      "2025-04-09 16:10:11,934 : INFO : EPOCH 21 - PROGRESS: at 20.32% examples, 479166 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:12,960 : INFO : EPOCH 21 - PROGRESS: at 28.33% examples, 481374 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:13,973 : INFO : EPOCH 21 - PROGRESS: at 35.80% examples, 465030 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:14,978 : INFO : EPOCH 21 - PROGRESS: at 44.51% examples, 478911 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:10:15,992 : INFO : EPOCH 21 - PROGRESS: at 55.48% examples, 491611 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:10:16,996 : INFO : EPOCH 21 - PROGRESS: at 74.76% examples, 504881 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:18,005 : INFO : EPOCH 21 - PROGRESS: at 91.45% examples, 510985 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:10:19,021 : INFO : EPOCH 21 - PROGRESS: at 99.10% examples, 505149 words/s, in_qsize 9, out_qsize 0\n",
      "2025-04-09 16:10:19,103 : INFO : EPOCH 21: training on 6067715 raw words (4149702 effective words) took 8.2s, 507159 effective words/s\n",
      "2025-04-09 16:10:20,132 : INFO : EPOCH 22 - PROGRESS: at 19.55% examples, 433019 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:10:21,157 : INFO : EPOCH 22 - PROGRESS: at 26.12% examples, 416417 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:22,165 : INFO : EPOCH 22 - PROGRESS: at 33.31% examples, 417245 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:10:23,169 : INFO : EPOCH 22 - PROGRESS: at 41.67% examples, 437112 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:24,180 : INFO : EPOCH 22 - PROGRESS: at 49.93% examples, 446481 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:25,186 : INFO : EPOCH 22 - PROGRESS: at 68.78% examples, 464138 words/s, in_qsize 16, out_qsize 1\n",
      "2025-04-09 16:10:26,189 : INFO : EPOCH 22 - PROGRESS: at 87.58% examples, 475590 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:10:27,204 : INFO : EPOCH 22 - PROGRESS: at 96.23% examples, 483039 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:27,635 : INFO : EPOCH 22: training on 6067715 raw words (4148177 effective words) took 8.5s, 486995 effective words/s\n",
      "2025-04-09 16:10:28,649 : INFO : EPOCH 23 - PROGRESS: at 20.86% examples, 517763 words/s, in_qsize 14, out_qsize 0\n",
      "2025-04-09 16:10:29,671 : INFO : EPOCH 23 - PROGRESS: at 28.86% examples, 501351 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:30,683 : INFO : EPOCH 23 - PROGRESS: at 37.34% examples, 498218 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:10:31,706 : INFO : EPOCH 23 - PROGRESS: at 46.05% examples, 501379 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:32,724 : INFO : EPOCH 23 - PROGRESS: at 64.48% examples, 507549 words/s, in_qsize 14, out_qsize 2\n",
      "2025-04-09 16:10:33,743 : INFO : EPOCH 23 - PROGRESS: at 75.93% examples, 515324 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:34,743 : INFO : EPOCH 23 - PROGRESS: at 91.89% examples, 513922 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:10:35,691 : INFO : EPOCH 23: training on 6067715 raw words (4149461 effective words) took 8.0s, 515832 effective words/s\n",
      "2025-04-09 16:10:36,711 : INFO : EPOCH 24 - PROGRESS: at 20.46% examples, 486653 words/s, in_qsize 13, out_qsize 2\n",
      "2025-04-09 16:10:37,722 : INFO : EPOCH 24 - PROGRESS: at 28.91% examples, 501976 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:38,724 : INFO : EPOCH 24 - PROGRESS: at 37.55% examples, 504433 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:39,752 : INFO : EPOCH 24 - PROGRESS: at 46.10% examples, 503841 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:10:40,778 : INFO : EPOCH 24 - PROGRESS: at 64.15% examples, 504812 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:10:41,785 : INFO : EPOCH 24 - PROGRESS: at 74.99% examples, 506161 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:42,793 : INFO : EPOCH 24 - PROGRESS: at 91.23% examples, 508356 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:43,841 : INFO : EPOCH 24 - PROGRESS: at 98.61% examples, 499211 words/s, in_qsize 13, out_qsize 0\n",
      "2025-04-09 16:10:44,006 : INFO : EPOCH 24: training on 6067715 raw words (4149714 effective words) took 8.3s, 499568 effective words/s\n",
      "2025-04-09 16:10:45,034 : INFO : EPOCH 25 - PROGRESS: at 20.86% examples, 512996 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:10:46,042 : INFO : EPOCH 25 - PROGRESS: at 29.66% examples, 525656 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:47,048 : INFO : EPOCH 25 - PROGRESS: at 39.12% examples, 535024 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:48,073 : INFO : EPOCH 25 - PROGRESS: at 48.64% examples, 539904 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:49,080 : INFO : EPOCH 25 - PROGRESS: at 67.14% examples, 546053 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:50,083 : INFO : EPOCH 25 - PROGRESS: at 87.12% examples, 549065 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:51,091 : INFO : EPOCH 25 - PROGRESS: at 95.37% examples, 544889 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:51,690 : INFO : EPOCH 25: training on 6067715 raw words (4148820 effective words) took 7.7s, 541014 effective words/s\n",
      "2025-04-09 16:10:52,723 : INFO : EPOCH 26 - PROGRESS: at 20.50% examples, 489618 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:10:53,724 : INFO : EPOCH 26 - PROGRESS: at 28.22% examples, 482763 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:54,746 : INFO : EPOCH 26 - PROGRESS: at 36.97% examples, 490480 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:55,751 : INFO : EPOCH 26 - PROGRESS: at 45.45% examples, 494715 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:56,759 : INFO : EPOCH 26 - PROGRESS: at 56.55% examples, 500005 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:57,788 : INFO : EPOCH 26 - PROGRESS: at 74.67% examples, 502882 words/s, in_qsize 16, out_qsize 3\n",
      "2025-04-09 16:10:58,839 : INFO : EPOCH 26 - PROGRESS: at 92.17% examples, 513209 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:10:59,677 : INFO : EPOCH 26: training on 6067715 raw words (4149799 effective words) took 8.0s, 520485 effective words/s\n",
      "2025-04-09 16:11:00,687 : INFO : EPOCH 27 - PROGRESS: at 20.74% examples, 511221 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:01,708 : INFO : EPOCH 27 - PROGRESS: at 26.66% examples, 436244 words/s, in_qsize 12, out_qsize 3\n",
      "2025-04-09 16:11:02,716 : INFO : EPOCH 27 - PROGRESS: at 34.89% examples, 450588 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:03,723 : INFO : EPOCH 27 - PROGRESS: at 42.61% examples, 451528 words/s, in_qsize 16, out_qsize 0\n",
      "2025-04-09 16:11:04,736 : INFO : EPOCH 27 - PROGRESS: at 49.79% examples, 446187 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:05,743 : INFO : EPOCH 27 - PROGRESS: at 67.34% examples, 458215 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:06,755 : INFO : EPOCH 27 - PROGRESS: at 76.40% examples, 448168 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:07,777 : INFO : EPOCH 27 - PROGRESS: at 91.23% examples, 445513 words/s, in_qsize 11, out_qsize 4\n",
      "2025-04-09 16:11:08,805 : INFO : EPOCH 27 - PROGRESS: at 99.23% examples, 449268 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-09 16:11:08,857 : INFO : EPOCH 27: training on 6067715 raw words (4148459 effective words) took 9.2s, 452315 effective words/s\n",
      "2025-04-09 16:11:09,901 : INFO : EPOCH 28 - PROGRESS: at 20.09% examples, 457907 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:10,928 : INFO : EPOCH 28 - PROGRESS: at 28.21% examples, 473206 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:11:11,968 : INFO : EPOCH 28 - PROGRESS: at 37.34% examples, 487947 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:12,977 : INFO : EPOCH 28 - PROGRESS: at 46.22% examples, 498462 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:14,002 : INFO : EPOCH 28 - PROGRESS: at 64.36% examples, 500605 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:15,051 : INFO : EPOCH 28 - PROGRESS: at 73.97% examples, 488120 words/s, in_qsize 15, out_qsize 1\n",
      "2025-04-09 16:11:16,058 : INFO : EPOCH 28 - PROGRESS: at 89.82% examples, 488619 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:17,074 : INFO : EPOCH 28 - PROGRESS: at 97.31% examples, 485283 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:17,366 : INFO : EPOCH 28: training on 6067715 raw words (4148940 effective words) took 8.5s, 488261 effective words/s\n",
      "2025-04-09 16:11:18,384 : INFO : EPOCH 29 - PROGRESS: at 20.10% examples, 469416 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:19,419 : INFO : EPOCH 29 - PROGRESS: at 28.59% examples, 487867 words/s, in_qsize 16, out_qsize 3\n",
      "2025-04-09 16:11:20,425 : INFO : EPOCH 29 - PROGRESS: at 37.24% examples, 494128 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:21,439 : INFO : EPOCH 29 - PROGRESS: at 44.40% examples, 476791 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:22,452 : INFO : EPOCH 29 - PROGRESS: at 53.35% examples, 477380 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:23,470 : INFO : EPOCH 29 - PROGRESS: at 72.42% examples, 478269 words/s, in_qsize 15, out_qsize 0\n",
      "2025-04-09 16:11:24,475 : INFO : EPOCH 29 - PROGRESS: at 87.94% examples, 476961 words/s, in_qsize 16, out_qsize 2\n",
      "2025-04-09 16:11:25,489 : INFO : EPOCH 29 - PROGRESS: at 95.92% examples, 479294 words/s, in_qsize 14, out_qsize 1\n",
      "2025-04-09 16:11:25,993 : INFO : EPOCH 29: training on 6067715 raw words (4150516 effective words) took 8.6s, 481747 effective words/s\n",
      "2025-04-09 16:11:25,993 : INFO : Word2Vec lifecycle event {'msg': 'training on 182031450 raw words (124487660 effective words) took 217.0s, 573728 effective words/s', 'datetime': '2025-04-09T16:11:25.993734', 'gensim': '4.3.3', 'python': '3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.3.2-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-04-09 16:11:25,994 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=21323, vector_size=100, alpha=0.025>', 'datetime': '2025-04-09T16:11:25.994469', 'gensim': '4.3.3', 'python': '3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.3.2-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "tokenized_sentences = X  # This should be a list of lists of tokens\n",
    "\n",
    "# Train Word2Vec without bigrams\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    sg=1,\n",
    "    min_count=3,\n",
    "    workers=8,\n",
    "    epochs=30\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5bb1f9b-bc4c-44fd-88e1-ad8de3b0e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('alone', 0.7591335773468018), ('sad', 0.707220733165741), ('depressed', 0.7047163248062134), ('miserable', 0.6860513687133789), ('empty', 0.6572108268737793), ('hopeless', 0.6511791348457336), ('isolated', 0.6439523100852966), ('deppresed', 0.6405414938926697), ('unloved', 0.6365382671356201), ('downtrodden', 0.6357808709144592)]\n",
      "[('community', 0.5609440207481384), ('expanded', 0.5491969585418701), ('dss', 0.5194636583328247), ('health', 0.5176531076431274), ('realtime', 0.5124025344848633), ('partnered', 0.5076718926429749), ('network', 0.5042071342468262), ('services', 0.5002152323722839), ('selectivity', 0.4987241327762604), ('professional', 0.4979346990585327)]\n",
      "0.5070648\n",
      "happiness\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.most_similar(\"lonely\"))\n",
    "print(w2v_model.wv.most_similar(positive=[\"anxiety\", \"support\"], negative=[\"pain\"]))\n",
    "print(w2v_model.wv.similarity(\"therapy\", \"meditation\"))\n",
    "print(w2v_model.wv.doesnt_match([\"depression\", \"anxiety\", \"happiness\", \"ocd\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fd4e9bc-7bdb-4e0a-a73b-47b754df13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 0 for <PAD>, 1 for <UNK>\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "\n",
    "# Add all words from Word2Vec vocab starting from index 2\n",
    "word2idx.update({word: idx + 2 for idx, word in enumerate(w2v_model.wv.index_to_key)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "113c8915-f345-471a-97f8-53e5232d3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79c4cc9-525c-49b4-bfb5-a304154c2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f77929d-c25e-4ba7-a842-2a678a3a9ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "device = ('mps' if torch.backends.mps.is_available else 'cpu')\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(y_train),\n",
    "                                     y=y_train)\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a15e950-bc12-4f00-8f0f-7b2a3dc000ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdevice\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc5d5e86-e6c4-4df9-99e2-282136c949c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = w2v_model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
    "for word, idx in word2idx.items():\n",
    "     \n",
    "     if word in w2v_model.wv:\n",
    "          embedding_matrix[idx] = w2v_model.wv[word]\n",
    "\n",
    "def encode_sentence(tokens, word2idx):\n",
    "    return [word2idx.get(token, 1) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "923eb2b6-e971-48a7-8c82-956d03e6bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "# --- Modify Dataset ---\n",
    "class MentalHealthDataset(Dataset):\n",
    "    def __init__(self, X, y, word2idx): \n",
    "        self.X = [torch.tensor(encode_sentence(x, word2idx),dtype=torch.long) for x in X]\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# --- Define collate_fn ---\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = _text[:MAX_LEN]\n",
    "        text_list.append(processed_text)\n",
    "    padded_texts = pad_sequence(text_list, batch_first=True, padding_value=0).long()\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.long)\n",
    "    return padded_texts, labels\n",
    "\n",
    "\n",
    "train_dataset = MentalHealthDataset(X_train.tolist(), y_train, word2idx)\n",
    "val_dataset = MentalHealthDataset(X_test.tolist(), y_test, word2idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e136e-d44c-42bc-b9f7-40f83d4e7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=False \n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim * 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            embedded = self.embedding(x)\n",
    "            lstm_out, (hn, cn) = self.lstm(embedded)\n",
    "           \n",
    "            final_hidden = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "            return self.fc(final_hidden) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4411a62c-dcd5-41cf-b54f-f47fcac78fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 1.3568\n",
      "Epoch 1/20 - Val Loss: 1.0837 | Val Acc: 0.5885\n",
      "Epoch 2/20 - Loss: 0.9862\n",
      "Epoch 2/20 - Val Loss: 0.9575 | Val Acc: 0.6197\n",
      "Epoch 3/20 - Loss: 0.8505\n",
      "Epoch 3/20 - Val Loss: 0.8721 | Val Acc: 0.6524\n",
      "Epoch 4/20 - Loss: 0.7491\n",
      "Epoch 4/20 - Val Loss: 0.8943 | Val Acc: 0.6349\n",
      "Epoch 5/20 - Loss: 0.6760\n",
      "Epoch 5/20 - Val Loss: 0.8387 | Val Acc: 0.7128\n",
      "Epoch 6/20 - Loss: 0.6087\n",
      "Epoch 6/20 - Val Loss: 0.8334 | Val Acc: 0.7106\n",
      "Epoch 7/20 - Loss: 0.5466\n",
      "Epoch 7/20 - Val Loss: 0.8366 | Val Acc: 0.7212\n",
      "Epoch 8/20 - Loss: 0.5040\n",
      "Epoch 8/20 - Val Loss: 0.8807 | Val Acc: 0.7101\n",
      "Epoch 9/20 - Loss: 0.4526\n",
      "Epoch 9/20 - Val Loss: 0.8664 | Val Acc: 0.7325\n",
      "⛔ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BiLSTMClassifier(embedding_matrix, hidden_dim=128, output_dim=len(le.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "EPOCHS = 20\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_bilstm_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdcecfad-808f-44b4-8db6-3e7798253360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             anxiety       0.76      0.82      0.79       755\n",
      "             bipolar       0.75      0.77      0.76       527\n",
      "          depression       0.73      0.56      0.64      3016\n",
      "              normal       0.96      0.89      0.92      3308\n",
      "personality disorder       0.38      0.69      0.49       237\n",
      "              stress       0.50      0.55      0.52       536\n",
      "            suicidal       0.58      0.74      0.65      2158\n",
      "\n",
      "            accuracy                           0.73     10537\n",
      "           macro avg       0.66      0.72      0.68     10537\n",
      "        weighted avg       0.75      0.73      0.74     10537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f46bd8-80bf-4f6d-8616-196bfbe70730",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_index = X_train.index[0]\n",
    "print(\"Original tokens:\", X_bigrams[original_index])\n",
    "print(\"Encoded tokens:\", sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3257383-5958-471c-a05a-0cc026b94366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))                      # List? Series? DataFrame?\n",
    "print(type(X[1]))                   # List of tokens or something else?\n",
    "print(X[1][:10])                    # First 10 tokens of the first sentence\n",
    "print(encode_sentence(X[1], word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33765c80-39cf-4d94-8668-192250603c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca5535bc-dfc5-4903-afff-23bba3d032be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(tokens, max_len=50):\n",
    "    seq = [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [word2idx[\"<PAD>\"]] * (max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5b3f096-4d87-46e9-a688-de585c3b3c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted mental health condition: suicidal\n"
     ]
    }
   ],
   "source": [
    "text = \"Nope. No, I can't go any further. I am tired every day is a war with myself\"\n",
    "tokens = clean_text(text)\n",
    "sequence = text_to_sequence(tokens)\n",
    "input_tensor = torch.tensor(sequence).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    prediction = torch.argmax(output, dim=1).item()\n",
    "\n",
    "predicted_label = le.inverse_transform([prediction])[0]\n",
    "print(\"Predicted mental health condition:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92264c41-aac9-4ecc-a8c2-2b7c504836b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
